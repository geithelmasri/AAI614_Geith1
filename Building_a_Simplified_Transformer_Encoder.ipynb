{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geithelmasri/AAI614_Geith1/blob/main/Building_a_Simplified_Transformer_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Week 3 Hands-on Lab: Building a Simplified Transformer Encoder**"
      ],
      "metadata": {
        "id": "UvODY4XYGS4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hands-on lab allows you to understand the Transformer architecture by implementing a basic Transformer encoder. You will learn how input embeddings, positional encodings, and feedforward layers work together in an encoder block. We will be using the Torch framework to build a simple transformer encoder."
      ],
      "metadata": {
        "id": "tV7jIspzGcIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Input Embedding and Positional Encoding**"
      ],
      "metadata": {
        "id": "UD0bA42xGnZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tGenerate Input Data**\n",
        "Define a sample sentence and tokenize it into a numerical format.\n"
      ],
      "metadata": {
        "id": "B-uZW8_bGvUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Example sentence and token IDs (simplified for illustration)\n",
        "token_ids = torch.tensor([[1, 2, 3, 4, 5]])  # Tokenized sentence\n",
        "vocab_size = 10  # Vocabulary size\n",
        "embedding_dim = 8  # Embedding size"
      ],
      "metadata": {
        "id": "bulopUQ1GzrC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Create an Embedding Layer**\n",
        "Implement the embedding layer to convert token IDs into dense vectors."
      ],
      "metadata": {
        "id": "4khCooiqIdHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "embedded_tokens = embedding_layer(token_ids)\n",
        "print(\"Embedded Tokens:\\n\", embedded_tokens)"
      ],
      "metadata": {
        "id": "LjWIeI7GIoEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70bfcfdf-7783-4945-8ae5-12e5894e12b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded Tokens:\n",
            " tensor([[[-0.3395,  0.1483, -0.0963,  0.1969,  0.3873, -0.8638, -0.8736,\n",
            "           0.0853],\n",
            "         [-1.0874, -0.7826, -0.9653,  0.4712,  0.5091,  0.3613, -0.0342,\n",
            "           1.4888],\n",
            "         [ 0.2657,  0.3678,  1.8991, -2.1339, -0.2700, -0.4976, -2.3552,\n",
            "           1.1595],\n",
            "         [-0.7438, -1.0204,  0.0696, -0.8761,  1.7035,  0.3335,  0.5493,\n",
            "           0.6885],\n",
            "         [-0.0164,  1.9604, -0.5623,  0.2379, -0.3100, -0.1869, -0.2129,\n",
            "           0.7123]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.\tAdd Positional Encoding**\n",
        "Incorporate positional encoding to provide positional information to the model.\n"
      ],
      "metadata": {
        "id": "EfdPOMcpG_qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(seq_len, embedding_dim):\n",
        "    position = np.arange(seq_len)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, embedding_dim, 2) * -(np.log(10000.0) / embedding_dim))\n",
        "    pe = np.zeros((seq_len, embedding_dim))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    return torch.tensor(pe, dtype=torch.float)\n",
        "\n",
        "seq_len = token_ids.size(1)\n",
        "pos_encoding = positional_encoding(seq_len, embedding_dim)\n",
        "print(\"Positional Encoding:\\n\", pos_encoding)\n"
      ],
      "metadata": {
        "id": "Mti1h0tXHDXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb1d8033-8bf1-4256-a5e4-e30631aeef61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positional Encoding:\n",
            " tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "          1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
            "        [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
            "          9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
            "        [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
            "          9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
            "        [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9996e-02,\n",
            "          9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
            "        [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
            "          9.9920e-01,  4.0000e-03,  9.9999e-01]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the positional encoding to the embedded tokens:"
      ],
      "metadata": {
        "id": "iZm2hKp-HIxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_with_pos = embedded_tokens + pos_encoding.unsqueeze(0)\n",
        "print(\"Embedded Tokens with Positional Encoding:\\n\", embedded_with_pos)\n"
      ],
      "metadata": {
        "id": "TMwXiAIiHLm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49bdd9a0-ab93-4376-8720-3a0fe3672811"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded Tokens with Positional Encoding:\n",
            " tensor([[[-0.3395,  1.1483, -0.0963,  1.1969,  0.3873,  0.1362, -0.8736,\n",
            "           1.0853],\n",
            "         [-0.2459, -0.2423, -0.8655,  1.4662,  0.5191,  1.3612, -0.0332,\n",
            "           2.4888],\n",
            "         [ 1.1750, -0.0484,  2.0977, -1.1538, -0.2500,  0.5022, -2.3532,\n",
            "           2.1595],\n",
            "         [-0.6027, -2.0104,  0.3651,  0.0793,  1.7335,  1.3331,  0.5523,\n",
            "           1.6884],\n",
            "         [-0.7732,  1.3067, -0.1729,  1.1589, -0.2700,  0.8123, -0.2089,\n",
            "           1.7123]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Add a Feedforward Layer**"
      ],
      "metadata": {
        "id": "wfM3VX60HQ6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\t**Define a Feedforward Neural Network**\n",
        "Implement a simple feedforward layer as part of the encoder.\n"
      ],
      "metadata": {
        "id": "oUaKhCEuHViQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedforward = nn.Sequential(\n",
        "    nn.Linear(embedding_dim, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, embedding_dim)\n",
        ")\n",
        "ff_output = feedforward(embedded_with_pos)\n",
        "print(\"Feedforward Output:\\n\", ff_output)\n"
      ],
      "metadata": {
        "id": "RIN1MxZVHZwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d5f6a6-344b-4285-8853-693bf1136c58"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feedforward Output:\n",
            " tensor([[[ 0.4244, -0.2674, -0.0852,  0.0999, -0.0216,  0.0861, -0.2141,\n",
            "          -0.3497],\n",
            "         [ 0.6305, -0.1091, -0.0598,  0.0283,  0.2752, -0.0334, -0.0078,\n",
            "          -0.9175],\n",
            "         [ 0.5153, -0.2356, -0.2940,  0.1847,  0.0290, -0.0071, -0.4606,\n",
            "          -0.5978],\n",
            "         [ 0.8648, -0.0198, -0.1940, -0.1126,  0.4668, -0.0191, -0.0405,\n",
            "          -1.2217],\n",
            "         [ 0.4955, -0.2597, -0.1891,  0.1449,  0.0370,  0.0496, -0.1711,\n",
            "          -0.4550]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3: Combine the Components into an Encoder Block**"
      ],
      "metadata": {
        "id": "xb9uak0OHcjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\t**Define the Encoder Block**\n",
        "Combine the embedding, positional encoding, and feedforward components into an encoder block.\n"
      ],
      "metadata": {
        "id": "qXVdYvwkHgdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, embedding_dim)\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embedding(x)\n",
        "        pos_enc = positional_encoding(x.size(1), embed.size(2))\n",
        "        embed_with_pos = embed + pos_enc.unsqueeze(0)\n",
        "        ff_output = self.feedforward(embed_with_pos)\n",
        "        return self.layer_norm(embed_with_pos + ff_output)\n",
        "\n",
        "encoder = TransformerEncoderBlock(vocab_size, embedding_dim)\n",
        "output = encoder(token_ids)\n",
        "print(\"Encoder Output:\\n\", output)\n"
      ],
      "metadata": {
        "id": "70IMzzkPHmRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5877f6b6-4bb6-4f05-bd8a-0a321cb5bf85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Output:\n",
            " tensor([[[ 0.4006,  0.8551,  0.2504, -1.5370, -1.0545,  0.8056,  1.3405,\n",
            "          -1.0608],\n",
            "         [ 0.4120,  0.0219, -0.7859, -0.5379, -1.9107,  1.0950,  0.2965,\n",
            "           1.4089],\n",
            "         [-1.2847, -0.8070,  1.9712,  0.8418,  0.6551, -0.4991, -0.5816,\n",
            "          -0.2958],\n",
            "         [-1.5421, -0.4892, -0.6163,  0.7889, -1.1101,  1.3615,  0.7692,\n",
            "           0.8382],\n",
            "         [-1.2884,  1.2501, -0.3774,  1.5386, -0.3200, -1.3350, -0.0814,\n",
            "           0.6135]]], grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4: Experiment with Different Inputs**\n",
        "\n",
        "* Test with Different Sentences\n",
        "Replace token_ids with new examples to observe how the encoder processes different inputs.\n",
        "* Modify Hyperparameters\n",
        "Experiment with different embedding sizes, feedforward dimensions, or positional encoding scales to see their effect on the output.\n"
      ],
      "metadata": {
        "id": "zfB-L5zlHqhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example 1: A longer sentence\n",
        "token_ids_1 = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n",
        "output_1 = encoder(token_ids_1)\n",
        "print(\"Encoder Output with longer sentence:\\n\", output_1)\n",
        "\n",
        "# Example 2: A shorter sentence\n",
        "token_ids_2 = torch.tensor([[1, 2, 3]])\n",
        "output_2 = encoder(token_ids_2)\n",
        "print(\"Encoder Output with shorter sentence:\\n\", output_2)\n",
        "\n",
        "# Example 3: A sentence with repeated tokens\n",
        "token_ids_3 = torch.tensor([[1, 1, 2, 2, 3]])\n",
        "output_3 = encoder(token_ids_3)\n",
        "print(\"Encoder Output with repeated tokens:\\n\", output_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx215BiMEo_I",
        "outputId": "ee10b231-f49e-4dbe-fba6-4c555501005e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Output with longer sentence:\n",
            " tensor([[[ 0.4006,  0.8551,  0.2504, -1.5370, -1.0545,  0.8056,  1.3405,\n",
            "          -1.0608],\n",
            "         [ 0.4120,  0.0219, -0.7859, -0.5379, -1.9107,  1.0950,  0.2965,\n",
            "           1.4089],\n",
            "         [-1.2847, -0.8070,  1.9712,  0.8418,  0.6551, -0.4991, -0.5816,\n",
            "          -0.2958],\n",
            "         [-1.5421, -0.4892, -0.6163,  0.7889, -1.1101,  1.3615,  0.7692,\n",
            "           0.8382],\n",
            "         [-1.2884,  1.2501, -0.3774,  1.5386, -0.3200, -1.3350, -0.0814,\n",
            "           0.6135],\n",
            "         [-1.8068,  0.4872, -0.1251, -0.4395, -0.2045,  2.0094, -0.2817,\n",
            "           0.3611],\n",
            "         [-0.9836,  0.1381, -0.5481,  0.8788,  0.5302,  1.8115, -0.3180,\n",
            "          -1.5090],\n",
            "         [ 0.5170,  1.8270, -0.2839, -0.5932, -0.8515,  0.9909, -1.4978,\n",
            "          -0.1086]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "Encoder Output with shorter sentence:\n",
            " tensor([[[ 0.4006,  0.8551,  0.2504, -1.5370, -1.0545,  0.8056,  1.3405,\n",
            "          -1.0608],\n",
            "         [ 0.4120,  0.0219, -0.7859, -0.5379, -1.9107,  1.0950,  0.2965,\n",
            "           1.4089],\n",
            "         [-1.2847, -0.8070,  1.9712,  0.8418,  0.6551, -0.4991, -0.5816,\n",
            "          -0.2958]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "Encoder Output with repeated tokens:\n",
            " tensor([[[ 0.4006,  0.8551,  0.2504, -1.5370, -1.0545,  0.8056,  1.3405,\n",
            "          -1.0608],\n",
            "         [ 1.0930,  0.3010,  0.3149, -1.5156, -1.0711,  0.6805,  1.2581,\n",
            "          -1.0609],\n",
            "         [ 0.6011, -0.8641, -0.4618, -0.3532, -1.7779,  1.0831,  0.3172,\n",
            "           1.4557],\n",
            "         [ 0.0042, -1.2134, -0.2059, -0.1705, -1.5797,  1.1862,  0.4441,\n",
            "           1.5350],\n",
            "         [-2.0542, -0.5564,  1.5619,  0.6861,  0.7115, -0.2068, -0.0965,\n",
            "          -0.0456]]], grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# Modify Hyperparameters\n",
        "embedding_dim_list = [8, 16, 32]  # different embedding sizes\n",
        "ff_dim_list = [16, 32, 64]  # different feedforward dimensions\n",
        "pos_scale_list = [10000.0, 5000.0, 20000.0]  # different positional encoding scales\n",
        "\n",
        "for embedding_dim in embedding_dim_list:\n",
        "  for ff_dim in ff_dim_list:\n",
        "    for pos_scale in pos_scale_list:\n",
        "      print(f\"\\nExperiment with embedding_dim={embedding_dim}, ff_dim={ff_dim}, pos_scale={pos_scale}\")\n",
        "\n",
        "      # positional encoding with new scale\n",
        "      def positional_encoding_modified(seq_len, embedding_dim, scale):\n",
        "          position = np.arange(seq_len)[:, np.newaxis]\n",
        "          div_term = np.exp(np.arange(0, embedding_dim, 2) * -(np.log(scale) / embedding_dim))\n",
        "          pe = np.zeros((seq_len, embedding_dim))\n",
        "          pe[:, 0::2] = np.sin(position * div_term)\n",
        "          pe[:, 1::2] = np.cos(position * div_term)\n",
        "          return torch.tensor(pe, dtype=torch.float)\n",
        "\n",
        "      #  new encoder with modified hyperparameters\n",
        "      encoder_modified = TransformerEncoderBlock(vocab_size, embedding_dim)\n",
        "      encoder_modified.feedforward = nn.Sequential(\n",
        "          nn.Linear(embedding_dim, ff_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(ff_dim, embedding_dim)\n",
        "      )\n",
        "\n",
        "      # Test\n",
        "      output_modified = encoder_modified(token_ids)\n",
        "      print(\"Modified Encoder Output:\\n\", output_modified)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2US6jQ-VEpDV",
        "outputId": "9c52263f-c751-47ef-e37c-7c2a813e51e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Experiment with embedding_dim=8, ff_dim=16, pos_scale=10000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-1.1230, -1.3283, -0.0488,  0.1102, -0.9538,  1.5336,  1.0786,\n",
            "           0.7314],\n",
            "         [ 0.3642, -1.1404, -0.0900,  0.3431, -1.3979,  0.6861, -0.6580,\n",
            "           1.8929],\n",
            "         [-0.2039, -1.3532, -1.6873,  1.4019,  0.2283,  0.7110,  0.0335,\n",
            "           0.8696],\n",
            "         [ 1.3296, -0.7619,  0.7278,  0.7544,  0.1753,  0.5033, -1.8868,\n",
            "          -0.8417],\n",
            "         [-0.0370, -0.4359, -1.1212,  0.8535, -0.5538,  1.2891, -1.3857,\n",
            "           1.3909]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=8, ff_dim=16, pos_scale=5000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.2874,  0.3574, -0.0981, -1.7109,  0.7651,  1.9446, -0.3999,\n",
            "          -0.5708],\n",
            "         [ 1.2407, -0.6639,  0.6269, -0.8499, -1.5044,  0.7030, -0.7878,\n",
            "           1.2354],\n",
            "         [ 1.5458, -0.8047,  0.1134,  0.0747, -1.0863,  0.1619, -1.3695,\n",
            "           1.3648],\n",
            "         [-1.1717,  0.6742, -0.1544,  1.4629, -0.6293,  0.4240, -1.5775,\n",
            "           0.9717],\n",
            "         [-0.5578, -0.7592,  1.5573,  0.0782, -0.6266, -0.8987, -0.5698,\n",
            "           1.7766]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=8, ff_dim=16, pos_scale=20000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-1.2400,  1.0150, -0.9340, -0.4433,  0.6177,  1.8883, -0.5134,\n",
            "          -0.3903],\n",
            "         [-1.0840,  0.4970, -0.4239, -0.3803, -0.4559,  2.3028, -0.7932,\n",
            "           0.3374],\n",
            "         [ 0.6160, -1.1066, -0.3030,  0.1300, -0.5808,  2.1701, -1.1000,\n",
            "           0.1743],\n",
            "         [-0.8052, -1.4027, -1.3790,  1.2126, -0.0256,  0.5570,  0.8806,\n",
            "           0.9622],\n",
            "         [-2.1126,  0.1126,  0.8394,  0.8304, -1.0867,  0.7291,  0.6449,\n",
            "           0.0429]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=8, ff_dim=32, pos_scale=10000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[ 0.0335,  0.1391, -0.7318,  0.4067,  1.3707, -0.0386, -2.1231,\n",
            "           0.9436],\n",
            "         [ 0.2679, -1.1852, -0.9257, -0.4818,  0.7941,  0.0302, -0.6057,\n",
            "           2.1062],\n",
            "         [-0.5793, -1.9895, -0.6437,  0.9354,  1.2924,  0.2822,  0.8096,\n",
            "          -0.1071],\n",
            "         [ 0.8924, -1.0409, -0.2146,  0.9106, -0.2600,  0.3900, -1.8872,\n",
            "           1.2098],\n",
            "         [ 0.2163, -0.0048, -1.1178, -0.7525, -1.4836,  0.4775,  1.0520,\n",
            "           1.6130]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=8, ff_dim=32, pos_scale=5000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-1.1553, -0.0952, -0.8241,  1.3193,  0.6732,  0.9782, -1.5491,\n",
            "           0.6530],\n",
            "         [ 1.0937,  1.4988,  0.8614, -0.7957, -0.2761, -1.4616, -0.9827,\n",
            "           0.0624],\n",
            "         [ 0.3371, -1.4923,  1.6473, -0.4854,  1.2405, -0.1286, -1.0736,\n",
            "          -0.0451],\n",
            "         [ 0.1894,  0.2441, -1.4925,  1.7113,  0.2530,  0.3506, -1.5695,\n",
            "           0.3137],\n",
            "         [-0.7080, -0.5876,  1.3289,  1.1421, -1.6898,  1.0359, -0.1637,\n",
            "          -0.3578]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=8, ff_dim=32, pos_scale=20000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.9270,  0.6663, -1.5270,  2.0189, -0.3813,  0.1828,  0.2179,\n",
            "          -0.2506],\n",
            "         [ 0.3569,  1.5878, -1.9218,  0.5842, -0.0285,  0.4448, -1.0569,\n",
            "           0.0338],\n",
            "         [ 0.0736, -0.6325,  1.4178,  0.9591, -1.2288,  1.1900, -1.1684,\n",
            "          -0.6108],\n",
            "         [-1.7564, -0.8150, -0.7046,  0.1822,  0.8085,  1.2396, -0.1785,\n",
            "           1.2243],\n",
            "         [-1.0585, -0.4425,  1.0200,  1.1306, -1.2226,  1.4280,  0.0551,\n",
            "          -0.9100]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=8, ff_dim=64, pos_scale=10000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.8693, -0.0591, -1.6731,  1.5742, -0.8334,  0.4011,  0.5841,\n",
            "           0.8755],\n",
            "         [ 1.1773,  0.4860, -0.3531,  1.5201, -1.3315, -0.5658, -1.3073,\n",
            "           0.3743],\n",
            "         [ 0.5584, -1.1671, -1.5180,  0.4541, -0.7204,  0.7248,  0.0035,\n",
            "           1.6647],\n",
            "         [ 1.8140, -2.0682, -0.1198, -0.2530,  0.4684,  0.0641, -0.2036,\n",
            "           0.2980],\n",
            "         [-1.0073, -0.4311,  1.1998,  1.1409,  0.0280,  0.7139, -1.8700,\n",
            "           0.2258]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=8, ff_dim=64, pos_scale=5000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.8843,  0.8940, -1.3751,  1.1780, -1.4810,  0.6662,  0.4861,\n",
            "           0.5161],\n",
            "         [ 0.1029, -0.0167, -1.0507, -1.5818, -0.6054,  1.4195,  0.3656,\n",
            "           1.3666],\n",
            "         [ 0.7076,  0.1717, -1.2158,  1.3847, -0.2153,  0.5329, -1.8685,\n",
            "           0.5025],\n",
            "         [-0.5694, -1.4778, -0.5584, -0.2088, -0.6802,  1.6839,  0.5903,\n",
            "           1.2206],\n",
            "         [-0.0664, -1.9739,  0.0297, -0.4231, -0.4329,  0.1476,  1.4401,\n",
            "           1.2791]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=8, ff_dim=64, pos_scale=20000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.9429,  0.9078,  0.2664,  1.1443, -2.1285,  0.5989,  0.1285,\n",
            "           0.0255],\n",
            "         [ 0.3064, -1.1722, -0.9975,  0.1200,  0.8205,  0.1124, -1.0962,\n",
            "           1.9066],\n",
            "         [ 1.1435, -0.4149,  0.3107,  0.6547, -1.9616,  0.9073, -1.0680,\n",
            "           0.4283],\n",
            "         [ 0.0469, -0.9830, -1.0710,  1.2125, -1.2825, -0.2099,  0.9088,\n",
            "           1.3783],\n",
            "         [-1.6333, -1.3386,  0.3768,  0.1747, -0.5156,  1.1715,  0.5881,\n",
            "           1.1764]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=16, ff_dim=16, pos_scale=10000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.7397, -0.2851, -0.1609,  1.9654, -0.6477, -0.4082,  0.0654,\n",
            "           1.5845, -0.9766,  1.6483, -1.2110, -0.7493, -0.7721,  1.2343,\n",
            "          -0.7286,  0.1814],\n",
            "         [ 1.3933,  1.1577, -0.0798,  0.8163, -1.1110, -0.4103,  0.3179,\n",
            "          -0.2635, -1.3139,  0.5985, -2.2208,  0.4527, -0.8304,  1.5930,\n",
            "           0.0478, -0.1474],\n",
            "         [ 0.5717,  0.0540, -1.3126, -0.5594, -1.9044,  0.9877,  0.3504,\n",
            "           0.8535,  0.8690, -1.5456,  0.3293,  0.2759, -1.0701,  0.5171,\n",
            "           1.8372, -0.2537],\n",
            "         [-0.9335, -2.1671,  0.1123,  0.3757,  0.1374,  1.8593, -0.2598,\n",
            "          -0.4048,  1.3268, -0.2553, -1.5404,  0.7859,  0.3292,  0.4559,\n",
            "          -0.7543,  0.9327],\n",
            "         [-0.3046, -0.4777,  1.3038,  1.3679, -1.4350,  1.4777,  0.0447,\n",
            "          -2.2450, -1.0766,  0.2218,  0.6747, -0.2349,  0.1883,  0.6784,\n",
            "           0.4523, -0.6357]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=16, ff_dim=16, pos_scale=5000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-1.1039,  0.1506, -0.6325, -0.7759, -1.2885,  1.7589, -0.9760,\n",
            "           0.5537, -0.6242, -0.2633,  0.5673,  1.0145, -0.4996,  0.9887,\n",
            "          -0.8610,  1.9912],\n",
            "         [-0.1979, -0.8092, -1.3581, -0.7672, -0.5868,  0.7959, -1.0586,\n",
            "           1.0216, -0.6368, -1.4971,  0.2132,  1.9872,  0.0412,  1.0024,\n",
            "           0.4945,  1.3557],\n",
            "         [ 0.3443, -2.4495, -0.1479,  0.7023, -0.8597,  0.7091, -1.3569,\n",
            "           0.2219,  0.0621, -0.1140, -0.2523,  0.5418,  0.5551,  0.9793,\n",
            "          -0.8866,  1.9510],\n",
            "         [ 0.1628, -1.4347, -0.6494,  0.3297, -0.3582, -0.3040, -1.3308,\n",
            "           1.6659, -1.2630,  0.5890,  1.7166,  0.2323,  0.6612,  0.5826,\n",
            "          -1.4657,  0.8657],\n",
            "         [-0.4785, -0.3773,  0.8526, -0.1940, -1.7731,  2.3954,  0.6646,\n",
            "           1.6447,  0.1881,  0.4390, -0.7582, -0.7564,  0.0190, -0.5238,\n",
            "          -0.2937, -1.0485]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=16, ff_dim=16, pos_scale=20000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.4450,  1.7282,  0.3434, -0.6115, -0.3628,  0.5761, -1.3460,\n",
            "           1.3819,  0.1494, -0.2022, -0.2110, -1.2157, -0.9422,  1.0553,\n",
            "          -1.4566,  1.5587],\n",
            "         [-1.1510, -0.9801, -0.3603, -1.0549,  0.9930,  2.1613, -0.5904,\n",
            "          -0.6138, -0.2837,  0.4260,  0.7403,  1.0242, -0.2846, -1.0969,\n",
            "          -0.5536,  1.6246],\n",
            "         [ 1.3942, -0.5909, -0.1580,  0.3484, -0.5812,  1.4423,  0.3441,\n",
            "           0.6720, -0.6369,  1.0369,  0.4891, -0.9306, -1.7572, -0.4355,\n",
            "          -1.8194,  1.1828],\n",
            "         [ 0.4925, -2.7853, -0.2622, -0.3646,  0.6918, -0.7651, -0.0057,\n",
            "           1.0429,  0.1325,  1.7043, -0.9385,  0.8088, -0.4477,  0.7256,\n",
            "          -0.4951,  0.4658],\n",
            "         [-0.8590, -0.9488,  1.1101,  0.5359, -1.7591,  0.6215,  1.6314,\n",
            "           0.6841,  0.7556,  0.4472, -1.8671, -1.0504, -0.3201,  0.1746,\n",
            "          -0.0151,  0.8593]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=16, ff_dim=32, pos_scale=10000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.0466,  0.8156,  0.0656,  1.2605, -0.1495,  0.8685, -1.9963,\n",
            "          -0.5363, -1.4217,  0.0607,  1.2600, -1.2171,  1.5174,  0.3863,\n",
            "          -1.0533,  0.1861],\n",
            "         [-0.3777,  0.6912, -0.2023,  2.6302, -0.4479,  0.9396,  0.8874,\n",
            "          -0.1356, -1.6355,  0.9342, -1.4065, -0.1194, -0.6966, -0.4516,\n",
            "          -0.5460, -0.0635],\n",
            "         [-0.5296, -0.8632,  0.4149,  0.9128, -0.5149,  0.1935,  1.3353,\n",
            "           1.4189, -1.3274, -1.3525,  1.9490,  0.2059, -1.1856, -0.8869,\n",
            "           0.4461, -0.2162],\n",
            "         [ 0.0602, -1.1032,  0.9306,  0.5387, -0.6314,  0.7905, -0.1112,\n",
            "           2.0301, -0.5695, -0.6525,  0.4782,  0.4116,  1.4771, -1.0073,\n",
            "          -1.8827, -0.7592],\n",
            "         [-0.4651, -0.1873,  0.7750, -1.1065, -0.5688,  1.9185,  0.3535,\n",
            "          -0.2349, -2.0292,  0.5044, -1.4887, -0.0896,  1.3040,  0.8959,\n",
            "          -0.3159,  0.7347]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=16, ff_dim=32, pos_scale=5000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.7887,  1.2491,  0.8676,  0.5103, -1.0371,  0.8512, -1.7419,\n",
            "           0.5359, -1.1420,  0.3294,  0.4886, -0.9277, -0.2813,  1.6503,\n",
            "          -1.3329,  0.7692],\n",
            "         [-1.3783,  0.7806, -0.0633,  1.3925, -0.3055,  1.3358, -0.4546,\n",
            "           0.5476, -1.6640,  0.4850, -0.0104, -0.3411, -1.2635,  1.6148,\n",
            "          -1.2352,  0.5596],\n",
            "         [ 0.8415, -3.1486, -0.1984, -0.1938,  1.5113, -0.0545, -0.2622,\n",
            "          -0.5242,  0.9161,  0.6168,  0.0992,  0.0860, -0.7555, -0.1859,\n",
            "           0.4777,  0.7745],\n",
            "         [ 0.1730, -1.3989,  1.7058,  0.9418, -1.2332,  0.0097, -0.2570,\n",
            "          -0.5721, -1.7306, -0.0868, -0.3797,  2.1281,  0.1087, -0.1525,\n",
            "          -0.0251,  0.7685],\n",
            "         [-2.0552,  0.6106, -0.2652, -0.8919,  0.3938, -0.6816,  0.1189,\n",
            "           0.5254, -1.8069,  0.1780,  0.5698,  0.6742, -0.9695,  1.5362,\n",
            "           1.2795,  0.7839]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=16, ff_dim=32, pos_scale=20000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.2589, -0.1245,  0.5879,  0.5255, -1.8726, -1.3199,  0.8244,\n",
            "           0.9812, -1.9105,  0.0553,  0.6033,  0.6803,  0.8161,  0.7203,\n",
            "          -1.3142,  1.0062],\n",
            "         [ 0.7491, -0.2603,  0.0495, -0.0551, -1.6465, -0.5276,  0.8627,\n",
            "           1.1365, -2.0786,  1.4584, -0.5561, -0.0665, -1.1745,  0.5568,\n",
            "           0.1909,  1.3612],\n",
            "         [ 0.5212, -1.1107, -0.8060, -1.0103, -0.9093,  0.5954,  0.3442,\n",
            "          -0.7899,  0.3001,  1.5771, -0.1521,  1.3182, -2.0288,  1.4488,\n",
            "           0.1259,  0.5760],\n",
            "         [ 1.9313, -0.2894, -0.5139,  1.4119, -1.5523,  1.4591,  0.0288,\n",
            "          -0.4905, -0.9436, -0.3488,  1.4624, -0.3026, -0.7701,  0.1903,\n",
            "          -0.1529, -1.1196],\n",
            "         [ 0.0904, -0.5740, -0.1152, -0.2309,  0.8037,  1.2164,  1.8386,\n",
            "          -0.4636, -1.2537,  0.2722, -0.0301,  0.9399, -2.7048,  0.0180,\n",
            "           0.0276,  0.1655]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=16, ff_dim=64, pos_scale=10000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[ 1.1562e+00,  4.6613e-01, -9.2362e-01, -2.2531e-01, -1.1284e+00,\n",
            "           1.8311e+00, -8.0402e-02,  9.3051e-02, -2.4858e-01, -4.6847e-01,\n",
            "          -1.7654e+00,  1.3392e+00, -1.1744e+00,  1.2449e+00, -6.7996e-01,\n",
            "           5.6401e-01],\n",
            "         [ 5.0431e-01, -1.2818e-01,  1.9661e-01,  7.7707e-01, -3.5632e-01,\n",
            "          -1.4951e+00, -1.1416e+00,  5.9791e-01,  2.2053e+00, -8.4342e-02,\n",
            "          -1.7796e+00,  9.9290e-05, -3.3008e-01,  1.4608e+00, -8.0175e-01,\n",
            "           3.7492e-01],\n",
            "         [ 1.4135e-01, -1.2652e+00,  1.6761e-01,  4.4093e-01, -8.6827e-02,\n",
            "           8.8480e-01, -9.3082e-01,  6.5719e-01, -1.6298e+00,  6.0630e-01,\n",
            "          -5.6024e-01,  1.5278e+00, -1.3397e+00,  1.9677e+00, -8.2010e-01,\n",
            "           2.3885e-01],\n",
            "         [-8.8021e-01, -1.7892e+00, -7.6659e-01,  8.5729e-01, -1.3055e+00,\n",
            "           1.8661e+00, -7.8685e-01,  1.3220e+00,  6.4267e-01,  7.2024e-01,\n",
            "           4.4661e-01, -3.8412e-01,  3.2043e-02, -1.0431e-01, -8.9654e-01,\n",
            "           1.0263e+00],\n",
            "         [-5.7914e-01, -1.9668e+00,  5.8154e-01,  1.9007e-01,  1.6218e+00,\n",
            "          -3.6374e-01, -6.6812e-01,  1.2121e+00,  5.4513e-01,  6.1709e-01,\n",
            "          -1.9373e+00,  8.9399e-01,  8.5836e-01,  1.0294e-01, -2.7792e-01,\n",
            "          -8.2995e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=16, ff_dim=64, pos_scale=5000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.3400,  2.2696, -0.5979, -0.1258,  0.2501, -1.2300,  0.6678,\n",
            "          -0.8665, -0.6533,  1.5399, -0.4486, -1.2339, -0.3700,  0.9999,\n",
            "          -0.9103,  1.0491],\n",
            "         [ 0.9522,  0.4568, -0.6493, -0.8813, -1.0323, -1.9050, -0.5385,\n",
            "           1.1147, -1.2060,  0.5962, -0.1329, -0.4487,  0.5310,  1.9092,\n",
            "           0.0079,  1.2260],\n",
            "         [ 0.9248,  0.8397, -1.1915, -0.2505,  0.4929,  1.4413, -0.9443,\n",
            "          -0.9936, -0.3339, -0.2757, -0.5507,  2.0410, -0.9485,  1.2830,\n",
            "          -0.3318, -1.2023],\n",
            "         [-0.7453, -0.7413,  0.7132,  0.4623, -0.8325,  1.9847, -1.0523,\n",
            "           1.7792, -1.3480,  0.6377,  0.6072, -0.2998,  0.7728, -0.0332,\n",
            "          -0.6581, -1.2464],\n",
            "         [ 1.0981, -1.3567, -1.0453,  0.2887, -0.5031, -0.2394, -0.2806,\n",
            "           1.8717, -1.0299,  2.0924, -0.5804, -0.9923, -0.0985,  1.0199,\n",
            "           0.0276, -0.2720]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=16, ff_dim=64, pos_scale=20000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.8129,  0.3375, -1.2067,  1.5262,  0.4358,  2.1941,  1.0232,\n",
            "           0.4813, -1.3072, -0.7724, -1.0433, -0.5919, -1.0858,  0.2631,\n",
            "           0.2128,  0.3462],\n",
            "         [-0.8276,  1.8271,  0.2080,  0.7021,  0.4502, -0.0715, -0.4998,\n",
            "           0.7786, -0.3755,  0.4488, -0.5261,  0.2311, -2.6991, -0.8412,\n",
            "          -0.1087,  1.3035],\n",
            "         [ 1.6050, -1.3800, -0.1566,  1.6416, -0.3129,  1.8983, -1.4993,\n",
            "           0.6734, -1.0737, -0.8850, -0.0219, -0.6001, -0.1501,  0.1917,\n",
            "           0.1740, -0.1044],\n",
            "         [-0.9295, -0.7751,  1.5735,  0.0955, -1.1830,  0.2374,  0.2565,\n",
            "           0.0391, -2.1621,  0.0542,  0.0769,  0.9859,  2.1731, -0.3836,\n",
            "          -0.0276, -0.0311],\n",
            "         [-1.6038, -1.7931,  0.9157,  1.1477, -0.4288, -0.4701, -0.7278,\n",
            "          -0.7838,  0.3579,  0.4353, -0.0997,  0.8091, -0.3843,  2.1993,\n",
            "          -0.2608,  0.6874]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=32, ff_dim=16, pos_scale=10000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.1241,  1.0957, -1.9422,  0.2884,  0.0531, -0.2805, -0.0555,\n",
            "          -0.2213, -0.5599,  1.8697, -1.1055, -0.2011, -1.1807,  0.8855,\n",
            "          -0.8710, -0.7023,  0.3633,  1.4668,  0.4527,  0.6106, -1.3240,\n",
            "           0.5184, -0.4421,  1.0314, -2.0127,  0.0192, -1.3738,  0.6471,\n",
            "          -0.1251,  0.7635,  0.1846,  2.2718],\n",
            "         [ 0.7075,  0.9446, -0.0786,  0.5886, -0.0985,  0.5775, -0.0287,\n",
            "           0.0736, -2.7517,  0.4207, -1.5050, -0.6733, -1.5081, -0.1033,\n",
            "          -0.6822,  1.0704, -0.0772,  2.1736, -0.5833, -0.1222, -1.4699,\n",
            "           0.8427, -0.7878,  1.4668, -0.1980,  1.2123, -0.8479,  0.3356,\n",
            "           0.1067,  1.5227, -0.2216, -0.3059],\n",
            "         [ 1.3837,  0.1355,  1.3064,  0.4057, -1.4026, -0.1584, -0.7056,\n",
            "          -0.2634, -0.6236, -0.2891,  0.5413, -0.2010, -1.3400,  0.9666,\n",
            "           0.7765,  0.5324, -0.9631,  0.4339, -0.0446, -0.2469, -0.0824,\n",
            "           1.0748, -2.9473,  0.5240, -1.1954,  0.4294, -1.2036,  0.1293,\n",
            "           0.8366,  1.0964, -0.9244,  2.0190],\n",
            "         [-1.2957, -0.3877,  1.6284,  0.2631, -1.5981, -0.6640,  1.2448,\n",
            "           1.8831, -0.1423,  0.0886, -1.2336,  1.3663, -0.5164,  2.1747,\n",
            "          -0.7878, -0.0679, -1.0149, -0.8937, -0.5121,  1.7802, -1.6692,\n",
            "          -0.2162, -0.0693,  0.3626, -0.6117, -0.3311,  0.9478, -0.1728,\n",
            "          -0.4868,  0.4968, -0.0081,  0.4429],\n",
            "         [-0.5536, -0.5675,  0.2663,  0.0871,  1.1907,  0.6844, -1.7774,\n",
            "           0.0517, -1.3320,  1.1930, -0.3688, -0.9013, -1.3638,  1.1740,\n",
            "           0.2429,  0.7939, -0.2918, -0.1070,  0.2340,  1.2314, -0.9935,\n",
            "           0.9583,  0.0932,  1.9118, -0.1824,  0.1687, -2.4641,  0.9668,\n",
            "          -0.8798,  1.4696, -0.9653,  0.0305]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=32, ff_dim=16, pos_scale=5000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-1.4660,  0.3776, -0.1974,  1.1704, -0.8056,  0.6250, -0.8903,\n",
            "          -0.1484, -2.1400,  1.0477,  0.3163,  0.5541, -0.5193,  0.3899,\n",
            "           0.6493, -0.4267, -1.8009,  0.3783,  0.8950,  2.0647, -0.9174,\n",
            "           0.4572, -1.8988, -0.4611,  0.0617,  0.1418, -1.1879,  0.5762,\n",
            "          -0.0157,  1.7807,  0.4571,  0.9326],\n",
            "         [-0.3627, -0.8710,  0.1240,  0.5895, -1.0062, -0.6956, -0.2506,\n",
            "           0.5566,  0.2913, -0.7337,  0.0800,  2.0146, -0.5318,  1.1652,\n",
            "           0.5816,  0.2138,  0.0076,  2.4044,  0.9721, -0.5945,  0.6576,\n",
            "          -1.1483, -0.2020,  2.0187,  0.0039, -0.0200, -0.4064,  0.3960,\n",
            "          -2.1029, -1.7134, -0.2168, -1.2211],\n",
            "         [ 1.5085, -0.0600,  0.5196,  1.0728,  1.8414,  0.5577,  0.9406,\n",
            "           0.2007,  0.1984,  1.6010, -1.4513, -0.6389, -0.2096,  0.1996,\n",
            "           0.8861,  0.4242, -0.2452,  0.6129, -1.6795,  1.0484, -1.6032,\n",
            "           0.3636,  0.0772, -0.0160, -2.0235,  0.1436,  0.0687, -0.0628,\n",
            "          -0.5676, -0.3389, -2.0074, -1.3611],\n",
            "         [-0.1346, -2.4686, -0.9782,  0.7671, -0.1135, -1.0868,  1.0383,\n",
            "          -1.0797, -1.2623, -0.4631, -0.4366,  2.2189, -0.1194,  1.3170,\n",
            "          -1.8977,  0.2652,  0.2835,  0.3252,  0.7898,  1.4690, -0.5108,\n",
            "           0.2783, -0.2657, -0.5458,  0.2074,  1.2806, -0.9881,  1.2412,\n",
            "           0.2022, -0.3535,  0.5079,  0.5129],\n",
            "         [-2.1034, -2.2117,  0.5143, -1.0684, -0.0956, -0.6870,  2.2585,\n",
            "           0.9359, -0.0235, -0.2567, -0.4446,  0.9174,  0.2416, -0.0472,\n",
            "           0.2319, -1.0329, -0.8801,  1.2954, -1.2709,  0.4169, -0.7938,\n",
            "          -0.2658, -1.0432,  0.7520,  0.8022,  0.3701,  0.1216,  0.3738,\n",
            "          -0.1340,  0.2213,  0.8559,  2.0499]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=32, ff_dim=16, pos_scale=20000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[ 0.4294,  1.7603,  1.3280, -0.0836,  0.2797,  0.6321, -1.1417,\n",
            "          -0.0069, -1.1901,  1.1453,  0.2730,  0.9957, -1.1317,  0.4785,\n",
            "          -2.2548, -1.3401, -0.5950,  0.9503, -1.6288, -0.4052, -0.2337,\n",
            "          -0.5505, -1.7117, -0.1646,  0.2359, -0.5274,  0.2685,  0.9022,\n",
            "           1.1715,  1.6642, -0.0284,  0.4798],\n",
            "         [ 1.5237,  0.5268, -0.2701,  0.5206,  0.3492, -0.2418, -0.3171,\n",
            "          -1.5281,  0.0098, -1.8972,  0.0216,  0.6207,  0.2772,  0.7920,\n",
            "           0.2002,  1.2630, -0.4316,  0.0213, -0.9242,  1.0006,  1.9878,\n",
            "           0.4606,  0.1646,  0.6710, -1.6974,  1.5279, -2.6509, -0.0891,\n",
            "          -0.7042, -0.4021, -0.7216, -0.0631],\n",
            "         [-1.8826,  0.0859, -0.3444, -0.2330, -0.1837, -0.5361,  1.0834,\n",
            "          -0.8719,  0.4396, -0.7694,  0.3869,  0.8196, -0.5341, -0.7602,\n",
            "           1.4281,  2.6141, -0.8185, -0.8203,  0.1347,  1.4374, -0.2593,\n",
            "           0.2313, -2.0509,  0.1230, -1.2454,  0.0712,  0.6186,  1.3000,\n",
            "          -1.3776,  0.4824,  0.9653,  0.4658],\n",
            "         [-1.3709, -1.0343,  0.1071,  0.0288,  0.6872,  1.0024, -0.8415,\n",
            "          -0.9929, -0.6675,  0.1829, -1.8233,  1.7918, -0.4397,  1.9871,\n",
            "           1.0359,  1.5242,  1.6608,  1.1014, -0.5887, -0.4272, -0.9495,\n",
            "          -0.8535, -0.4323,  0.9335, -0.3487, -0.7595, -1.5102,  0.3108,\n",
            "           0.2902,  0.7096, -0.6119,  0.2976],\n",
            "         [-1.4383, -1.1540, -0.8638,  0.7319,  0.6564, -0.0305,  1.7968,\n",
            "          -0.4095,  0.0398,  0.3014, -1.2436,  0.4552, -0.7793, -0.1246,\n",
            "           1.3068, -0.1273, -1.2247, -1.0974,  2.0252,  0.3225,  0.6108,\n",
            "           0.4928, -0.2882,  1.4223,  0.2735,  1.3850, -0.5373,  1.4935,\n",
            "          -1.2274, -1.5838, -0.1245, -1.0595]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=32, ff_dim=32, pos_scale=10000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-1.1642,  0.5700, -2.2580,  1.0939,  0.5452,  0.3492, -2.2648,\n",
            "           1.3576, -1.4645,  0.4647, -0.8905,  0.8622, -0.0355,  0.2597,\n",
            "          -0.2096, -0.3918,  0.5489, -0.3556, -0.9896,  1.6026,  0.4278,\n",
            "           0.7325,  0.3926, -0.3835, -0.0228, -0.2863,  1.0278,  0.9501,\n",
            "          -1.3095,  1.6757, -0.9469,  0.1124],\n",
            "         [-0.0687,  0.3448,  1.3888,  1.6204, -0.9600, -0.6365,  1.2455,\n",
            "          -0.6331,  0.0538,  0.8573, -0.3364,  0.9566,  0.2289, -0.0988,\n",
            "           0.2214,  0.1534, -1.4457,  1.0010, -0.6138,  0.6193,  1.1282,\n",
            "           1.4418, -2.1828, -0.4153, -0.7311,  1.4346, -1.6343, -1.2064,\n",
            "          -1.3281, -0.0737, -1.0195,  0.6884],\n",
            "         [-0.0108, -0.6590, -1.0581,  0.5117, -0.2691,  1.6606, -0.3450,\n",
            "           0.0369,  0.5964, -0.2978,  0.1716, -0.2674, -0.2988,  0.7171,\n",
            "          -0.7811,  1.4652, -1.1575,  1.1183,  0.0028, -1.2735,  1.2585,\n",
            "           0.9887, -0.4848,  1.7613, -0.2024, -1.1417, -0.8148,  1.8010,\n",
            "          -0.4490, -0.2738, -2.7387,  0.4334],\n",
            "         [-0.1655, -0.4315, -0.3867, -0.7132, -0.6729,  0.5230, -1.0224,\n",
            "           0.7629, -0.3968,  0.9275, -2.6200, -0.6918, -0.7279,  0.0329,\n",
            "          -0.4829,  0.8012,  1.8747,  0.4822,  0.1701,  1.0752,  0.6672,\n",
            "           1.8573,  0.0343, -0.5859,  0.7561, -1.7698, -0.8425, -0.0624,\n",
            "           0.4597,  0.6112, -1.2630,  1.7998],\n",
            "         [ 1.3951, -2.3797, -0.3138, -1.4381,  0.6827,  0.6945, -0.6578,\n",
            "           0.1096,  0.5672,  1.6472, -0.9827,  0.9060, -1.5739,  1.0837,\n",
            "          -0.2166,  0.9660, -0.7955,  0.1721,  0.5152, -1.0407,  0.5480,\n",
            "           1.1836, -0.0851, -0.4626,  0.4619,  0.2029, -1.1722,  1.6067,\n",
            "          -1.7864,  0.1631, -0.0906,  0.0905]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=32, ff_dim=32, pos_scale=5000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[ 1.4699e+00, -1.0204e-01, -1.0170e+00,  1.5330e+00, -4.1291e-02,\n",
            "           3.6807e-01, -1.1611e+00,  1.2258e-01, -6.0200e-01,  1.4848e+00,\n",
            "          -4.7978e-01, -2.9268e-02, -1.0282e+00,  1.6120e-01, -5.1806e-01,\n",
            "           8.0116e-01, -3.9488e-01,  3.0251e+00, -1.5546e+00,  7.5234e-01,\n",
            "          -1.7147e+00,  1.1653e+00, -9.5816e-02, -1.0807e-03,  3.7223e-01,\n",
            "          -3.9220e-01, -2.2864e-01,  2.3177e-01, -7.0870e-01,  6.1540e-01,\n",
            "          -1.3400e+00, -6.9346e-01],\n",
            "         [-6.0406e-01, -5.0977e-01, -1.0143e+00,  5.7065e-01, -7.6381e-01,\n",
            "          -2.3256e-01,  2.8421e-01,  1.0613e-01,  2.8003e-01,  1.5542e+00,\n",
            "          -2.5029e-02,  1.5057e+00, -8.1704e-01, -3.9741e-01, -6.8430e-01,\n",
            "          -4.7808e-01,  4.9758e-01,  1.4143e-01, -2.4508e+00,  2.5440e-01,\n",
            "           1.6194e-01,  8.7507e-01, -1.9355e+00, -1.5190e-01, -3.4552e-02,\n",
            "           7.9694e-01,  1.2844e+00,  1.2897e-01,  9.8618e-01,  2.1832e+00,\n",
            "          -1.9648e+00,  4.5282e-01],\n",
            "         [-1.2292e+00, -1.9088e+00, -1.1897e+00,  4.8120e-01, -1.6190e-01,\n",
            "          -1.0938e-01,  3.2384e-01,  1.1949e+00,  1.4322e-01,  1.4045e+00,\n",
            "           2.1080e-01, -3.9166e-01, -2.5819e+00,  1.7973e+00, -1.7637e+00,\n",
            "           1.4403e+00,  1.1583e-01,  7.9888e-01,  3.8754e-01, -8.7774e-02,\n",
            "          -2.9924e-01,  1.6975e+00, -3.1090e-01, -4.7871e-01, -2.3043e-01,\n",
            "          -5.8630e-01, -7.1363e-01,  7.4421e-01,  4.7505e-02,  5.5897e-01,\n",
            "           8.4067e-03,  6.8831e-01],\n",
            "         [-9.8091e-01, -1.4969e+00,  5.8904e-01, -7.1231e-01,  1.8083e-01,\n",
            "          -1.3207e+00,  1.4658e+00,  1.2103e+00, -1.4670e+00,  1.0098e+00,\n",
            "          -1.4748e+00,  1.3920e+00, -7.0173e-01,  8.3121e-01, -1.6576e+00,\n",
            "          -8.2258e-01, -1.7639e-01,  6.8985e-01,  4.8796e-01, -8.8736e-01,\n",
            "           9.8858e-01,  6.5828e-01, -7.9050e-01,  9.6277e-02,  2.8545e-01,\n",
            "           6.1880e-01,  4.8947e-01,  8.7829e-01, -1.7558e+00,  9.6601e-01,\n",
            "           1.5471e-01,  1.2520e+00],\n",
            "         [-2.2217e+00, -1.0613e+00, -1.4591e+00, -2.8500e-01, -1.3245e-01,\n",
            "          -1.0092e+00, -3.3114e-01, -1.5215e-03,  1.1851e+00, -4.5266e-01,\n",
            "          -1.2205e+00,  1.5338e+00, -1.6526e-01,  1.5388e+00, -2.6130e-01,\n",
            "           1.0331e+00, -5.1715e-02,  2.1955e-01, -1.1732e+00,  4.1958e-01,\n",
            "          -9.0652e-01,  2.5167e-01, -7.3440e-01,  1.3381e+00, -1.4459e+00,\n",
            "           6.3930e-01,  2.4741e-01,  9.5072e-01,  3.1240e-01,  1.7281e+00,\n",
            "           2.1220e-02,  1.4940e+00]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=32, ff_dim=32, pos_scale=20000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[ 1.8593e+00,  1.9534e+00,  6.7487e-01, -9.0763e-01,  4.4769e-01,\n",
            "           4.5164e-01, -2.0718e-01,  5.1528e-02,  2.6451e-01, -5.6832e-01,\n",
            "          -5.2322e-01,  1.1525e+00,  8.9547e-01,  4.2949e-01, -2.5376e+00,\n",
            "          -6.7193e-01,  2.8778e-01,  7.7438e-02, -9.9543e-01, -4.7556e-01,\n",
            "          -2.1056e-02,  4.6780e-01, -2.2837e+00, -6.4198e-01, -1.1224e+00,\n",
            "           1.0199e+00,  1.5644e-01,  5.6057e-01,  6.3145e-01,  1.0185e+00,\n",
            "          -1.3761e+00, -6.8105e-02],\n",
            "         [-1.7573e+00, -8.8894e-04, -7.0731e-01,  8.1235e-01,  3.5470e-01,\n",
            "           8.3224e-01,  3.5856e-01,  1.7031e-01,  4.3978e-01,  1.5351e+00,\n",
            "          -1.0292e+00, -7.3671e-01,  4.3288e-01,  2.9714e-01, -1.5639e+00,\n",
            "          -6.0622e-01, -9.5504e-01,  1.8785e+00, -3.5113e-01,  1.3457e+00,\n",
            "          -6.7857e-01, -1.9838e-01, -1.5486e+00,  4.4899e-01, -9.0216e-01,\n",
            "           1.2693e+00,  6.5747e-01,  1.4492e-01, -1.3397e+00,  2.2425e+00,\n",
            "          -3.9996e-01, -4.4538e-01],\n",
            "         [-8.2541e-01, -9.0089e-01,  7.0258e-02, -4.7316e-01, -2.0171e+00,\n",
            "           1.7812e-01,  4.6221e-02, -3.1635e-01,  1.8091e-02, -2.6619e-01,\n",
            "          -1.4993e+00,  1.3168e+00, -7.2999e-01,  1.4455e+00,  1.4018e-01,\n",
            "           1.1738e+00,  8.9277e-01, -3.5187e-01, -7.6073e-01,  1.4342e+00,\n",
            "          -1.0976e+00,  4.2888e-01, -5.1638e-01,  5.2734e-01, -1.0812e+00,\n",
            "           1.6458e+00,  6.5385e-01, -1.5557e+00, -4.0341e-01, -2.4942e-01,\n",
            "           8.8833e-01,  2.1847e+00],\n",
            "         [ 9.0551e-01, -1.6707e+00,  7.2125e-01, -1.3050e+00,  4.6227e-01,\n",
            "          -9.2342e-01,  8.6951e-01,  4.6527e-02,  1.6556e+00,  1.3459e+00,\n",
            "          -1.2762e-01,  2.3216e-01, -1.4619e-01, -1.6737e-01, -5.6737e-01,\n",
            "           5.3878e-01, -1.3006e+00, -1.5988e+00, -1.8099e+00,  9.6338e-01,\n",
            "          -1.3043e-01,  3.9449e-01, -3.7078e-01,  2.2724e+00, -9.5379e-01,\n",
            "           8.4079e-01, -8.7134e-01,  4.8827e-01, -6.9879e-01,  9.0699e-01,\n",
            "           7.7566e-01, -7.7725e-01],\n",
            "         [-1.6769e+00, -9.9434e-01, -1.3904e-01, -1.6252e+00, -3.9507e-01,\n",
            "          -9.0488e-01, -9.3003e-01, -6.2187e-01,  2.3013e+00,  1.0950e+00,\n",
            "           9.6077e-01,  1.3329e+00,  7.0391e-01, -7.3822e-01,  5.2836e-01,\n",
            "           6.8432e-01,  3.6550e-02,  9.3200e-01, -2.7211e-01,  7.9452e-01,\n",
            "           2.3303e-02,  1.4366e+00, -4.1916e-01,  1.0344e+00, -5.2495e-01,\n",
            "          -8.8931e-01,  6.2124e-01,  4.5940e-01, -1.5709e+00, -4.2328e-01,\n",
            "           7.7311e-01, -1.5925e+00]]], grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=32, ff_dim=64, pos_scale=10000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.6406, -0.5894,  0.2408,  0.6618,  1.3818,  2.0316,  0.1829,\n",
            "           0.4378, -0.2864,  0.1883, -1.8878,  2.1395,  0.0611, -1.3209,\n",
            "          -0.7183, -0.0979, -0.0356, -1.0000, -1.3258,  1.7559,  0.1766,\n",
            "          -0.9315, -0.0518,  0.8924, -0.2873, -0.2115, -0.4006,  0.8319,\n",
            "          -2.0917,  0.8907, -0.3611,  0.3648],\n",
            "         [ 0.7445, -0.2851,  0.1496,  0.0343, -1.0695,  1.1314, -1.8866,\n",
            "          -0.5551,  0.0229, -0.6491, -0.5012,  0.7559, -0.6620,  1.1906,\n",
            "          -0.3873,  0.4903, -0.8955,  1.5244, -1.5010, -1.2095, -1.0066,\n",
            "           1.3797,  0.7426,  1.9412,  1.7403,  0.8739, -1.8533, -0.0950,\n",
            "          -0.4189, -0.0537,  0.2496,  0.0580],\n",
            "         [ 0.4728, -0.3692, -0.5906, -1.3117, -0.7684,  1.1539,  0.1461,\n",
            "           1.5622, -0.1309, -0.3144, -0.6254, -0.6591, -2.4539,  0.8616,\n",
            "           1.4041, -1.0699, -0.5301,  1.3495, -2.0951, -0.1150, -0.7121,\n",
            "           0.0940,  0.6658,  1.4061, -0.7487,  1.7992,  0.4879,  0.6744,\n",
            "          -0.3181,  0.1726,  0.0131,  0.5494],\n",
            "         [-0.2384, -1.5155,  0.1644, -1.1113,  0.6172,  0.9238,  0.5548,\n",
            "          -0.5373, -0.4731,  1.7531, -0.4726,  0.3576, -1.4960,  0.8880,\n",
            "           0.3406,  0.4001, -1.2392,  1.5487,  0.2043,  1.6859, -2.3629,\n",
            "           1.1450,  1.3346, -1.1365, -0.7280,  0.2759,  0.1212, -1.3545,\n",
            "           0.5007,  0.1517, -0.0043, -0.2978],\n",
            "         [-2.0733, -1.9527,  0.2374, -1.5769,  1.2323,  1.2414,  1.3059,\n",
            "          -0.2969, -0.6433,  0.1021, -1.2492, -0.0031,  0.6533,  1.2192,\n",
            "          -0.7230,  1.1754, -1.5492,  0.9047,  0.3052, -0.8623,  0.8858,\n",
            "           0.0671, -1.0007,  0.4446, -0.4267, -0.7559,  0.5719,  0.4150,\n",
            "           0.4869,  0.6773, -0.4044,  1.5921]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=32, ff_dim=64, pos_scale=5000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-0.6448,  1.0885,  0.8841,  1.2362, -1.7369,  1.8274, -0.9341,\n",
            "           0.3441, -2.2150,  1.4859, -0.7726,  0.0528, -0.3514,  0.0358,\n",
            "           0.2096,  0.4323, -1.1949, -0.5672, -0.4348,  0.1474,  0.3456,\n",
            "           0.0964, -1.6366, -0.3174, -1.1321,  0.0956, -0.7135,  1.1800,\n",
            "          -0.2855,  1.7310,  0.7938,  0.9505],\n",
            "         [ 0.4581, -0.4078,  0.0314,  1.3134, -0.2764, -0.1081, -1.2976,\n",
            "           0.6307,  0.3052,  0.1580, -1.5844,  0.9816,  0.3804,  0.5704,\n",
            "           0.7938, -0.0745, -2.6354,  1.3835,  1.0297, -1.0938, -1.6836,\n",
            "           1.6306, -0.1210, -0.3277, -0.9953,  0.6677,  0.4980,  1.7144,\n",
            "          -0.8920,  0.1170, -0.2784, -0.8878],\n",
            "         [-0.7833, -1.7661, -0.7124, -0.4768,  0.0977,  0.2727,  1.3745,\n",
            "           0.8465,  0.2633,  0.7820, -0.7545, -0.3438, -0.6869,  0.8769,\n",
            "          -0.3982,  0.9107, -1.2880, -0.7951,  0.6805,  0.3004, -1.3341,\n",
            "           0.3239, -1.9777,  1.1342,  0.3843,  2.1865, -0.6012,  2.0456,\n",
            "          -0.3364,  0.6005,  0.3046, -1.1304],\n",
            "         [ 0.7996, -2.3910, -0.2172, -0.4352, -0.0468,  0.4426,  0.2996,\n",
            "           1.9019,  1.2319, -1.1795, -1.6606,  1.1875,  0.0308,  0.2805,\n",
            "           0.7674, -1.2967, -1.6476,  0.5812, -0.4721, -0.7255, -0.1092,\n",
            "           0.3209,  0.4932,  0.9009,  0.4641,  1.2591, -0.9332, -0.4881,\n",
            "           0.4962,  1.2059, -1.5130,  0.4523],\n",
            "         [ 0.0256,  0.3997,  1.5043, -1.0184,  1.2233,  0.1170,  1.1319,\n",
            "           0.2283, -0.9411, -0.3155, -0.2581,  0.8544, -1.0190,  0.5057,\n",
            "          -0.6046, -0.7392, -0.4763,  0.8359, -2.5335,  0.1007, -1.3856,\n",
            "           0.8747,  0.0650,  0.7147, -0.7488,  2.6623, -1.4333,  0.0712,\n",
            "          -0.6268, -0.2870,  0.0920,  0.9807]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "\n",
            "Experiment with embedding_dim=32, ff_dim=64, pos_scale=20000.0\n",
            "Modified Encoder Output:\n",
            " tensor([[[-1.5931,  0.5806, -0.7444, -1.2739,  0.6062,  0.7735, -0.9440,\n",
            "           0.4812, -1.6497,  0.5933, -0.6865, -0.9334,  0.2185,  1.5576,\n",
            "           0.7070,  1.0301,  0.0224,  0.4606, -0.9960,  0.1425, -1.5675,\n",
            "           2.0088, -0.2441,  0.8984,  0.6335,  0.4035,  0.8567,  0.5075,\n",
            "          -1.0626,  0.4142, -2.0577,  0.8565],\n",
            "         [-0.0126,  0.2482, -1.6514, -0.7734,  1.4337,  0.1591, -0.3368,\n",
            "           0.0515, -0.0641,  1.1132, -1.1478,  1.7362,  0.6517,  0.5416,\n",
            "           0.0916, -0.2395, -1.1107, -1.0402,  1.2923, -0.5547, -0.0310,\n",
            "           0.4185, -1.9039,  0.8241, -0.4290,  1.3132, -1.7997, -0.1521,\n",
            "          -0.8896,  0.2428, -0.2606,  2.2792],\n",
            "         [-0.5622, -1.4500, -0.3579, -0.7330,  0.6192,  1.1629, -0.6048,\n",
            "           0.8261,  0.7089,  1.4084, -1.2856,  1.5453, -0.5924,  0.7437,\n",
            "          -1.1691, -0.2601, -1.4084, -1.3425,  0.9235,  0.4411, -0.1765,\n",
            "          -0.1186, -1.0496, -0.0959, -0.4103,  1.3107, -1.3307, -0.2420,\n",
            "           1.1500,  1.8874, -0.9026,  1.3648],\n",
            "         [-0.0387, -1.0269, -0.4277,  0.5105,  0.9895,  1.2256, -0.2054,\n",
            "           0.7889, -0.8091, -1.6436,  1.0734,  1.3034,  0.0627,  0.3893,\n",
            "          -0.2366,  0.4604, -0.2178,  1.1319, -0.9283, -0.0585, -1.5436,\n",
            "           2.4682, -1.0234, -0.2562,  0.9357,  0.6417, -2.2506, -1.2924,\n",
            "          -0.7159,  0.6361,  0.0533,  0.0042],\n",
            "         [-0.2538, -1.2460,  0.7938, -0.8592,  1.5447,  0.5435, -0.2188,\n",
            "          -0.2798, -1.4663,  0.6898, -0.1188,  0.6505, -0.8005,  0.2669,\n",
            "          -2.2568,  1.6464, -0.7767, -0.3198, -0.5494,  1.4789, -0.1880,\n",
            "           0.2520,  0.9794, -0.8563, -1.0520, -1.0129, -0.1105,  0.0933,\n",
            "          -0.8585,  1.1745,  1.0386,  2.0720]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Summary**"
      ],
      "metadata": {
        "id": "IZW4m09oIIFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By completing this lab, you have:\n",
        "\n",
        "* Understood the role of embedding, positional encoding, and feedforward layers in the Transformer encoder.\n",
        "* Gained hands-on experience implementing a core component of the Transformer architecture.\n",
        "* Developed a deeper appreciation for the architectures design and functionality.\n",
        "\n",
        "This lab builds foundational knowledge of the Transformer, preparing you for more advanced concepts like self-attention.\n"
      ],
      "metadata": {
        "id": "49Ko4E9cH4df"
      }
    }
  ]
}