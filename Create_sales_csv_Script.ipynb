{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geithelmasri/AAI614_Geith1/blob/main/Create_sales_csv_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Define the data for the CSV file\n",
        "data = [\n",
        "    [\"transaction_id\", \"customer_id\", \"product_id\", \"quantity\", \"price\"],\n",
        "    [\"T001\", \"C001\", \"P001\", 2, 100],\n",
        "    [\"T002\", \"C002\", \"P002\", 1, 200],\n",
        "    [\"T003\", \"C001\", \"P003\", 3, 50],\n",
        "]\n",
        "\n",
        "# Define the filename\n",
        "filename = \"sales.csv\"\n",
        "\n",
        "# Write the data to the CSV file\n",
        "try:\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        csv_writer = csv.writer(csvfile)\n",
        "        csv_writer.writerows(data)\n",
        "    print(f\"Successfully created '{filename}'\")\n",
        "except IOError as e:\n",
        "    print(f\"Error writing to file '{filename}': {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created 'sales.csv'\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djd6sBG_3tob",
        "outputId": "559de2ba-b0d5-4e1f-d89d-ac7bc6868d5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "def extract_data_from_csv(filename):\n",
        "\n",
        "  try:\n",
        "    df = pd.read_csv(filename)\n",
        "    return df\n",
        "  except FileNotFoundError:\n",
        "    logging.error(f\"Error: File not found at '{filename}'\")\n",
        "    raise\n",
        "  except Exception as e:\n",
        "    logging.error(f\"Error reading CSV file '{filename}': {e}\")\n",
        "    raise\n",
        "\n",
        "# Example usage (assuming sales.csv is created from the previous code)\n",
        "try:\n",
        "    sales_df = extract_data_from_csv(filename)\n",
        "    print(\"\\nSuccessfully extracted data:\")\n",
        "    print(sales_df.head())\n",
        "except (FileNotFoundError, Exception) as e:\n",
        "    print(f\"Data extraction failed due to: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwSw51zf33br",
        "outputId": "9b71b8ae-fcee-4235-96d2-1103dadb1e2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully extracted data:\n",
            "  transaction_id customer_id product_id  quantity  price\n",
            "0           T001        C001       P001         2    100\n",
            "1           T002        C002       P002         1    200\n",
            "2           T003        C001       P003         3     50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(df):\n",
        "\n",
        "  df['total_revenue'] = df['quantity'] * df['price']\n",
        "  return df\n",
        "\n",
        "# Example usage\n",
        "try:\n",
        "    transformed_df = transform_data(sales_df.copy()) # Use a copy to avoid modifying the original DataFrame\n",
        "    print(\"\\nSuccessfully transformed data:\")\n",
        "    print(transformed_df.head())\n",
        "except NameError:\n",
        "    print(\"Error: sales_df is not defined. Please run the extraction code first.\")\n",
        "except Exception as e:\n",
        "    print(f\"Data transformation failed due to: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z89wobna4C_M",
        "outputId": "72e98ba8-d321-4c11-cb97-886c7e78249e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully transformed data:\n",
            "  transaction_id customer_id product_id  quantity  price  total_revenue\n",
            "0           T001        C001       P001         2    100            200\n",
            "1           T002        C002       P002         1    200            200\n",
            "2           T003        C001       P003         3     50            150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSY6tExs4dzh",
        "outputId": "acdbcca6-5593-424d-b7ea-01790d1c2f7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading pymongo-4.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.7.0 pymongo-4.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "from pymongo.errors import ConnectionFailure, OperationFailure\n",
        "\n",
        "# MongoDB connection string\n",
        "mongodb_uri = \"mongodb+srv://user1:ecommerce1@ecommerce.mvmspnu.mongodb.net/?retryWrites=true&w=majority&appName=ecommerce\"\n",
        "database_name = \"ecommerce\"\n",
        "collection_name = \"sales\"\n",
        "\n",
        "def load_data_to_mongodb(df, uri, db_name, collection_name):\n",
        "\n",
        "  client = None\n",
        "  try:\n",
        "    client = MongoClient(uri)\n",
        "    # The ismaster command is cheap and does not require auth.\n",
        "    client.admin.command('ismaster')\n",
        "    db = client[db_name]\n",
        "    collection = db[collection_name]\n",
        "\n",
        "    # Convert DataFrame to a list of dictionaries (JSON-like documents)\n",
        "    data_to_insert = df.to_dict('records')\n",
        "\n",
        "    # Insert the data into the collection\n",
        "    if data_to_insert:\n",
        "      result = collection.insert_many(data_to_insert)\n",
        "      print(f\"\\nSuccessfully inserted {len(result.inserted_ids)} documents into MongoDB collection '{collection_name}'\")\n",
        "    else:\n",
        "      print(\"\\nNo data to insert into MongoDB.\")\n",
        "\n",
        "  except ConnectionFailure as e:\n",
        "    logging.error(f\"Could not connect to MongoDB: {e}\")\n",
        "  except OperationFailure as e:\n",
        "    logging.error(f\"MongoDB operation failed: {e}\")\n",
        "  except Exception as e:\n",
        "    logging.error(f\"An unexpected error occurred during MongoDB load: {e}\")\n",
        "  finally:\n",
        "    if client:\n",
        "      client.close()\n",
        "      print(\"MongoDB connection closed.\")\n",
        "\n",
        "\n",
        "# Example usage (assuming transformed_df is available from the previous code)\n",
        "try:\n",
        "    if 'transformed_df' in locals():\n",
        "        load_data_to_mongodb(transformed_df, mongodb_uri, database_name, collection_name)\n",
        "    else:\n",
        "        print(\"Error: transformed_df is not defined. Please run the transformation code first.\")\n",
        "except Exception as e:\n",
        "    print(f\"Data loading to MongoDB failed due to: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u182mm_C4R9S",
        "outputId": "9af0c93e-48bc-43c6-fde8-fce2c91ca5b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Successfully inserted 3 documents into MongoDB collection 'sales'\n",
            "MongoDB connection closed.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import logging\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import ConnectionFailure, OperationFailure\n",
        "import os # Import the os module\n",
        "\n",
        "# Remove any existing handlers from the root logger to prevent duplicate logs\n",
        "# This is important if this cell is run multiple times\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "# Configure logging to write to a file and console\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,  # Set logging level to INFO for general progress\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    filename='pipeline.log',  # Log to a file named pipeline.log\n",
        "    filemode='w'  # Overwrite the log file each time\n",
        ")\n",
        "\n",
        "# Get the root logger\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# Add a stream handler to also print logs to the console\n",
        "# Check if a console handler already exists to avoid adding multiple\n",
        "if not any(isinstance(handler, logging.StreamHandler) for handler in logger.handlers):\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "    console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "\n",
        "def extract_data_from_csv(filename):\n",
        "  logger.info(f\"Attempting to extract data from '{filename}'\")\n",
        "  try:\n",
        "    df = pd.read_csv(filename)\n",
        "    logger.info(f\"Successfully extracted data from '{filename}'. Shape: {df.shape}\")\n",
        "    return df\n",
        "  except FileNotFoundError:\n",
        "    logger.error(f\"Error: File not found at '{filename}'\")\n",
        "    raise\n",
        "  except Exception as e:\n",
        "    logger.error(f\"Error reading CSV file '{filename}': {e}\")\n",
        "    raise\n",
        "\n",
        "def transform_data(df):\n",
        "  logger.info(\"Attempting to transform data.\")\n",
        "  try:\n",
        "    # Check if 'quantity' and 'price' columns exist before transformation\n",
        "    if 'quantity' not in df.columns or 'price' not in df.columns:\n",
        "        logger.error(\"Required columns 'quantity' or 'price' not found for transformation.\")\n",
        "        raise ValueError(\"Missing required columns for transformation\")\n",
        "    df['total_revenue'] = df['quantity'] * df['price']\n",
        "    logger.info(\"Successfully transformed data: 'total_revenue' column added.\")\n",
        "    return df\n",
        "  except Exception as e:\n",
        "    logger.error(f\"Data transformation failed: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "def load_data_to_mongodb(df, uri, db_name, collection_name):\n",
        "  logger.info(f\"Attempting to load data to MongoDB database '{db_name}' collection '{collection_name}'\")\n",
        "  client = None\n",
        "  try:\n",
        "    client = MongoClient(uri)\n",
        "    # The ismaster command is cheap and does not require auth.\n",
        "    client.admin.command('ismaster')\n",
        "    logger.info(\"Successfully connected to MongoDB.\")\n",
        "    db = client[db_name]\n",
        "    collection = db[collection_name]\n",
        "\n",
        "    data_to_insert = df.to_dict('records')\n",
        "\n",
        "    if data_to_insert:\n",
        "      result = collection.insert_many(data_to_insert)\n",
        "      logger.info(f\"Successfully inserted {len(result.inserted_ids)} documents into MongoDB collection '{collection_name}'\")\n",
        "    else:\n",
        "      logger.warning(\"No data to insert into MongoDB.\")\n",
        "\n",
        "  except ConnectionFailure as e:\n",
        "    logger.error(f\"Could not connect to MongoDB: {e}\")\n",
        "    raise  # Re-raise the exception to be caught by the main try block\n",
        "  except OperationFailure as e:\n",
        "    logger.error(f\"MongoDB operation failed: {e}\")\n",
        "    raise # Re-raise the exception\n",
        "  except Exception as e:\n",
        "    logger.error(f\"An unexpected error occurred during MongoDB load: {e}\")\n",
        "    raise # Re-raise the exception\n",
        "  finally:\n",
        "    if client:\n",
        "      client.close()\n",
        "      logger.info(\"MongoDB connection closed.\")\n",
        "\n",
        "# Define MongoDB credentials and database/collection names\n",
        "mongodb_uri = \"mongodb+srv://user1:ecommerce1@ecommerce.mvmspnu.mongodb.net/?retryWrites=true&w=majority&appName=ecommerce\"\n",
        "database_name = \"ecommerce\"\n",
        "collection_name = \"sales\"\n",
        "\n",
        "# Define filename for CSV\n",
        "filename = \"sales.csv\"\n",
        "\n",
        "# Orchestrate the pipeline\n",
        "try:\n",
        "    logger.info(\"Starting ETL pipeline.\")\n",
        "\n",
        "    # --- Extraction ---\n",
        "    # Ensure filename is defined in this scope if not already\n",
        "    if 'filename' not in locals() and 'filename' not in globals():\n",
        "        filename = \"sales.csv\" # Assuming default name if not globally defined\n",
        "        logger.warning(f\"Filename not defined in current scope, using default: {filename}\")\n",
        "\n",
        "    sales_df = extract_data_from_csv(filename)\n",
        "\n",
        "    # --- Transformation ---\n",
        "    # Ensure sales_df is defined before attempting transformation\n",
        "    if 'sales_df' not in locals():\n",
        "        logger.error(\"sales_df not defined. Extraction step failed.\")\n",
        "        raise NameError(\"sales_df not defined\")\n",
        "    transformed_df = transform_data(sales_df.copy())\n",
        "\n",
        "    # --- Loading ---\n",
        "    # Ensure transformed_df is defined before attempting loading\n",
        "    if 'transformed_df' not in locals():\n",
        "        logger.error(\"transformed_df not defined. Transformation step failed.\")\n",
        "        raise NameError(\"transformed_df not defined\")\n",
        "    load_data_to_mongodb(transformed_df, mongodb_uri, database_name, collection_name)\n",
        "\n",
        "    logger.info(\"ETL pipeline completed successfully.\")\n",
        "\n",
        "except (FileNotFoundError, ConnectionFailure, OperationFailure, NameError, ValueError, Exception) as e:\n",
        "    logger.error(f\"ETL pipeline failed due to an error: {e}\")\n",
        "\n",
        "# Close the log file handlers to ensure data is written\n",
        "for handler in logger.handlers:\n",
        "    if isinstance(handler, logging.FileHandler):\n",
        "        handler.close()\n",
        "\n",
        "# Download the log file only if it exists\n",
        "log_file_path = 'pipeline.log'\n",
        "if os.path.exists(log_file_path):\n",
        "    print(f\"\\nDownloading {log_file_path}...\")\n",
        "    files.download(log_file_path)\n",
        "    print(f\"{log_file_path} download initiated.\")\n",
        "else:\n",
        "    print(f\"\\nLog file '{log_file_path}' not found. Pipeline might have failed very early.\")\n",
        "    # Optionally print logs that might have been written to console\n",
        "    print(\"\\nConsole output might contain error details.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "cdx0iSH0_tFd",
        "outputId": "c589c0f0-772e-4921-94fe-8d327370bdbb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading pipeline.log...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_489ad2dd-0ef1-47dd-a47f-a868c8e9092e\", \"pipeline.log\", 788)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pipeline.log download initiated.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}